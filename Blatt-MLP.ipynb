{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c1a200",
   "metadata": {},
   "source": [
    "# Übungsblatt: Overfitting & MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1fb795",
   "metadata": {},
   "source": [
    "## NN.MLP.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037a08fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1=0, x2=4 -> 1\n",
      "x1=3, x2=0 -> 1\n",
      "x1=0, x2=0 -> -1\n",
      "x1=3, x2=4 -> 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sign(x):\n",
    "    return 1 if x >= 0 else -1\n",
    "\n",
    "def perceptron_net(x1, x2):\n",
    "    h1 = sign(x2 - 3)\n",
    "    h2 = sign(x1 - 2)\n",
    "\n",
    "    y = sign(h1 + h2 + 0.5)\n",
    "    return y\n",
    "\n",
    "test_points = [\n",
    "    (0, 4),   # +1 (oben)\n",
    "    (3, 0),   # +1 (rechts)\n",
    "    (0, 0),   # -1 (außerhalb)\n",
    "    (3, 4)    # +1 (beides)\n",
    "]\n",
    "\n",
    "for x1, x2 in test_points:\n",
    "    print(f\"x1={x1}, x2={x2} -> {perceptron_net(x1, x2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26bc5c9",
   "metadata": {},
   "source": [
    "# NN.MLP.02:\n",
    "\n",
    "## Dimensionen der Gewichtsmatrizen und Bias-Vektoren\n",
    "\n",
    "Gewichte:\n",
    "\n",
    "- $W^{[1]} \\in \\mathbb{R}^{64 \\times 25}$\n",
    "- $W^{[2]} \\in \\mathbb{R}^{32 \\times 64}$\n",
    "- $W^{[3]} \\in \\mathbb{R}^{4 \\times 32}$\n",
    "\n",
    "Bias:\n",
    "\n",
    "- $b^{[1]} \\in \\mathbb{R}^{64}$\n",
    "- $b^{[2]} \\in \\mathbb{R}^{32}$\n",
    "- $b^{[3]} \\in \\mathbb{R}^{4}$\n",
    "\n",
    "## Vorwärtslauf in Matrix-Notation\n",
    "\n",
    "Sei $x \\in \\mathbb{R}^{25}$ der Eingabevektor. Dann gilt:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{[0]} &= x \\\\\n",
    "z^{[1]} &= W^{[1]} a^{[0]} + b^{[1]}, \\quad a^{[1]} = \\mathrm{ReLU}(z^{[1]}) \\\\\n",
    "z^{[2]} &= W^{[2]} a^{[1]} + b^{[2]}, \\quad a^{[2]} = \\mathrm{ReLU}(z^{[2]}) \\\\\n",
    "z^{[3]} &= W^{[3]} a^{[2]} + b^{[3]}, \\quad a^{[3]} = \\mathrm{ReLU}(z^{[3]})\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125a51d",
   "metadata": {},
   "source": [
    "# NN.MLP.03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a3f489",
   "metadata": {},
   "source": [
    "## 01:\n",
    "\n",
    "### 1. Datensatz: Kreis (Circle)\n",
    "* **Entscheidungsgrenze:** Eine gerade Linie, die durch die konzentrischen Kreise schneidet. Sie kann die innere Punktwolke nicht umschließen.\n",
    "* **Kosten & Anpassung:** Test-Loss ist sehr hoch (0.528). Es liegt klares Underfitting vor.\n",
    "* **Geschwindigkeit:** Schnell (110 Epochen), da das Modell simpel ist, aber es lernt keine Lösung.\n",
    "* **Korrekte Klassifizierung:** Nein. Ein Kreis ist nicht linear separierbar. Ohne nicht-lineare Features oder Hidden Layers ist eine Lösung unmöglich.\n",
    "\n",
    "### 2. Datensatz: Exklusives Oder (XOR)\n",
    "* **Entscheidungsgrenze:** Eine gerade Linie, die versucht, diagonal gegenüberliegende Cluster zu trennen, was geometrisch unmöglich ist.\n",
    "* **Kosten & Anpassung:** Test-Loss ~0.5 (Raten). Starkes Underfitting.\n",
    "* **Geschwindigkeit:** Sehr schnell (53 Epochen), das Modell \"gibt auf\".\n",
    "* **Korrekte Klassifizierung:** Nein. XOR ist das klassische Beispiel für nicht linear separierbare Daten.\n",
    "\n",
    "### 3. Datensatz: Gaussian (Zwei Cluster)\n",
    "* **Entscheidungsgrenze:** Eine gerade Linie trennt die beiden Punktwolken perfekt.\n",
    "* **Kosten & Anpassung:** Test-Loss ist 0.000. Perfektes Modell (\"Good Fit\"), kein Overfitting.\n",
    "* **Geschwindigkeit:** 315 Epochen (länger, um den Fehler auf absolut Null zu drücken).\n",
    "* **Korrekte Klassifizierung:** Ja. Dieser Datensatz ist linear separierbar; eine gerade Linie reicht völlig aus.\n",
    "\n",
    "### 4. Datensatz: Spirale (Spiral)\n",
    "* **Entscheidungsgrenze:** Eine gerade Linie schneidet willkürlich durch die Spiralarme.\n",
    "* **Kosten & Anpassung:** Hoher Test-Loss (0.481). Starkes Underfitting.\n",
    "* **Geschwindigkeit:** 89 Epochen.\n",
    "* **Korrekte Klassifizierung:** Nein. Spiralen sind hochgradig nicht-linear. Das Modell ist viel zu einfach für diese Komplexität."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a662f84",
   "metadata": {},
   "source": [
    "# 02:\n",
    "# Vergleichsanalyse: 1 Hidden Layer (2 Neuronen)\n",
    "\n",
    "### 1. Wie verhält sich die Entscheidungsgrenze?\n",
    "* **Strukturelles Limit:** Mit nur 2 Neuronen kann das Netzwerk mathematisch nur zwei lineare Trennlinien kombinieren.\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * Eine geschlossene Form (Kreis) ist unmöglich, da man dafür mindestens 3 Geraden (für ein Dreieck) bräuchte, um eine Fläche einzuschließen.\n",
    "    * **ReLU:** Erzeugt eine eckige, nach oben offene \"V\"-Form.\n",
    "    * **Tanh / Sigmoid:** Erzeugen einen glatten, offenen \"Kanal\" oder Streifen.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * Die Entscheidungsgrenze scheitert komplett und zeigt meist nur eine leichte Kurve oder zwei fast parallele Linien. Die komplexe Struktur der Spirale kann nicht erfasst werden.\n",
    "\n",
    "### 2. Trainings- und Testkosten & Überanpassung\n",
    "* **Ergebnis:** In allen Fällen liegt massives Underfitting vor. Das Modell ist zu simpel (\"High Bias\").\n",
    "* **Werte (Test Loss):**\n",
    "    * *Kreis:* ~0.24 bis 0.29 (Sigmoid schnitt hier am \"besten\" ab, löst das Problem aber nicht).\n",
    "    * *Spirale:* ~0.47 bis 0.52 (Das ist fast so schlecht wie bloßes Raten bei 0.5).\n",
    "* **Überanpassung:** Tritt nicht auf, da das Modell nicht einmal die Trainingsdaten lernen kann.\n",
    "\n",
    "### 3. Geschwindigkeit der Berechnung\n",
    "* **Konvergenz:** Sehr schnell (oft unter 150 Epochen stabil).\n",
    "* **Grund:** Das Modell stößt extrem schnell an seine kapazitäre Grenze. Es findet die \"bestmögliche schlechte Lösung\" und lernt dann nicht mehr weiter.\n",
    "\n",
    "### 4. Korrektheit der Klassifizierung\n",
    "* **Nein.**\n",
    "    * Beim Kreis werden Randbereiche falsch klassifiziert, da die Form nicht geschlossen werden kann.\n",
    "    * Bei der Spirale werden fast alle Datenpunkte falsch getrennt, da 2 Neuronen nicht die nötigen Zick-Zack-Bewegungen der Spiralarme abbilden können.\n",
    "\n",
    "### 5. Analyse der Hidden Layers (Versteckte Schichten)\n",
    "* **Funktionsweise:** Die zwei Neuronen in der versteckten Schicht lernen jeweils eine einzelne Trennlinie (bzw. bei Sigmoid/Tanh einen weichen Übergang).\n",
    "* **Vergleich Output:** Die letzte Schicht (Output) addiert lediglich diese zwei simplen Komponenten.\n",
    "    * *Unterschied:* Man sieht deutlich, dass ReLU harte Kanten liefert, während Sigmoid/Tanh weichere Übergänge schaffen. Das Grundproblem der fehlenden Komplexität (zu wenig Neuronen) können aber alle Aktivierungsfunktionen nicht kompensieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6765b",
   "metadata": {},
   "source": [
    "# Vergleichsanalyse: 1 Hidden Layer (3 Neuronen)\n",
    "\n",
    "### 1. Wie verhält sich die Entscheidungsgrenze?\n",
    "* **Struktureller Durchbruch:** Mit 3 Neuronen stehen dem Netzwerk 3 Linien zur Verfügung. Geometrisch reicht das aus, um eine geschlossene Form (ein Dreieck) zu bilden.\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **Erfolg:** Das Netzwerk schafft es endlich, die inneren blauen Punkte vollständig zu umschließen.\n",
    "    * **Form:** Bei ReLU sieht man ein scharfkantiges Dreieck. Bei Tanh und Sigmoid ist es ein abgerundetes, weiches Dreieck (\"Blob\"), das die Daten sehr gut trennt.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **Misserfolg:** Die 3 Linien reichen bei weitem nicht aus, um den komplexen Windungen der Spirale zu folgen. Die Grenze bleibt zu einfach.\n",
    "\n",
    "### 2. Trainings- und Testkosten & Überanpassung\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **Good Fit:** Der Test-Loss sinkt massiv auf Werte um 0.010 bis 0.022. Das Modell hat das Problem gelöst. Es liegt kein Overfitting vor, sondern eine gute Generalisierung.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **Underfitting:** Der Loss bleibt sehr hoch (ca. 0.46 bis 0.50). Das Modell ist weiterhin zu einfach für diese Datenstruktur (High Bias).\n",
    "\n",
    "### 3. Geschwindigkeit der Berechnung\n",
    "* **Verhalten:** Die Berechnung stabilisiert sich zügig (ca. 200–300 Epochen).\n",
    "* **Unterschied:** Beim Kreis konvergiert das Netz gegen eine echte Lösung (globales Minimum). Bei der Spirale stagniert es, weil mit der begrenzten Kapazität keine Verbesserung möglich ist.\n",
    "\n",
    "### 4. Korrektheit der Klassifizierung\n",
    "* **Kreis:** Ja. Fast alle Punkte werden korrekt klassifiziert. Die blauen Punkte sind erfolgreich \"eingesperrt\".\n",
    "* **Spirale:** Nein. Die Klassifizierung ist kaum besser als Raten, da die Spiralarme ignoriert werden.\n",
    "\n",
    "### 5. Analyse der Hidden Layers (Versteckte Schichten)\n",
    "* **Funktionsweise:** Die 3 Neuronen lernen jeweils eine Gerade, die an den Rändern der blauen Punktwolke entlangläuft.\n",
    "* **Kombination:** Das Output-Neuron kombiniert diese drei \"Schnitte\". Wo sich die aktiven Bereiche der drei Geraden überschneiden, entsteht die \"Insel\" für die blaue Klasse.\n",
    "* **Erkenntnis:** Dies beweist geometrisch, dass man mindestens 3 Neuronen (ein Dreieck) benötigt, um eine Fläche in einem 2D-Raum vollständig zu umschließen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a2d93",
   "metadata": {},
   "source": [
    "# Vergleichsanalyse: 1 Hidden Layer (5 Neuronen)\n",
    "\n",
    "### 1. Wie verhält sich die Entscheidungsgrenze?\n",
    "* **Strukturelle Kapazität:** Mit 5 Neuronen stehen dem Netzwerk 5 lineare Schnitte zur Verfügung. Das erlaubt die Bildung eines Fünfecks (Polygon), was eine sehr gute Annäherung an einen Kreis ist.\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **Erfolg:** Das Netzwerk bildet eine geschlossene Form, die den inneren blauen Cluster fast perfekt umschließt.\n",
    "    * **Form:** Bei ReLU erkennt man deutlich ein eckiges Polygon (Fünfeck). Bei Tanh und Sigmoid sind die Ecken abgerundet, was zu einer fast perfekten Kreisform führt.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **Misserfolg:** Trotz der 2 zusätzlichen Neuronen (im Vergleich zu vorher) reicht die Kapazität nicht aus. Die 5 Linien können die vielfachen Windungen der Spirale nicht nachzeichnen. Die Grenze bleibt zu grob.\n",
    "\n",
    "### 2. Trainings- und Testkosten & Überanpassung\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **Good Fit:** Der Test-Loss ist extrem niedrig (0.010 bei ReLU, 0.016 bei Tanh). Das Problem ist gelöst. Es gibt kein Overfitting.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **Underfitting:** Der Loss stagniert bei hohen Werten (~0.48 bis 0.52). Das Modell leidet weiterhin an High Bias, da es die Datenstruktur nicht erfassen kann.\n",
    "\n",
    "### 3. Geschwindigkeit der Berechnung\n",
    "* **Kreis:** Das Modell konvergiert zügig und stabil zu einer optimalen Lösung.\n",
    "* **Spirale:** Die Berechnung erreicht sehr schnell ein Plateau, da mit nur 5 Geraden keine bessere geometrische Lösung möglich ist.\n",
    "\n",
    "### 4. Korrektheit der Klassifizierung\n",
    "* **Kreis:** Ja. Die Trennung ist präzise. Die 5 Neuronen reichen aus, um den blauen Bereich vom orangen Bereich zu isolieren.\n",
    "* **Spirale:** Nein. Große Teile der Spiralarme werden falsch klassifiziert, da das Modell versucht, mit wenigen geraden Schnitten durch eine geschwungene Struktur zu gehen.\n",
    "\n",
    "### 5. Analyse der Hidden Layers (Versteckte Schichten)\n",
    "* **Funktionsweise:** Jedes der 5 Neuronen definiert eine Kante des Polygons.\n",
    "* **Visualisierung:** Wenn man die Maus über die Neuronen hält, würde man 5 verschiedene Geraden sehen, die aus unterschiedlichen Winkeln auf das Zentrum zeigen.\n",
    "* **Synthese:** Das Output-Neuron kombiniert diese 5 \"Wände\", um den inneren Raum einzuzäunen. Für den Kreis ist das perfekt. Für die Spirale bräuchte man jedoch dutzende solcher Neuronen, um den Schlauch der Spirale Segment für Segment nachzubilden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb54248",
   "metadata": {},
   "source": [
    "# Vergleichsanalyse: 2 Hidden Layers (5 Neuronen pro Schicht)\n",
    "\n",
    "### 1. Wie verhält sich die Entscheidungsgrenze?\n",
    "* **Strukturelle Tiefe:** Durch die zweite Schicht kann das Netzwerk die 5 Linien aus der ersten Schicht komplexer kombinieren (z.B. XOR-Logik auf Formen anwenden).\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **Erfolg:** Die Form ist extrem präzise und umschließt die blauen Punkte perfekt.\n",
    "    * **Unterschied:** Im Vergleich zu nur einer Schicht wirken die Grenzen bei ReLU noch schärfer und \"selbstbewusster\". Bei Tanh/Sigmoid entsteht eine fast mathematisch perfekte Kreisform.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **Misserfolg:** Trotz der zweiten Schicht scheitert das Netzwerk. Die Entscheidungsgrenze sieht chaotisch aus oder bildet nur eine einfache Kurve. Die Spirale erfordert viel mehr \"Knicke\" (Neuronen), als die ersten 5 Neuronen bereitstellen können.\n",
    "\n",
    "### 2. Trainings- und Testkosten & Überanpassung\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **Perfektion:** Der Test-Loss sinkt auf extrem niedrige Werte (bis zu 0.002 bei Tanh). Das ist fast null Fehler. Kein Overfitting.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **Stagnation:** Der Loss bleibt hoch (ca. 0.47 bis 0.48). Das Hinzufügen einer zweiten Schicht hat das Underfitting-Problem bei dieser geringen Neuronenanzahl nicht gelöst.\n",
    "\n",
    "### 3. Geschwindigkeit der Berechnung\n",
    "* **Kreis:** Konvergiert sehr schnell. Das Netzwerk findet die Lösung fast augenblicklich.\n",
    "* **Spirale:** Das Training stagniert früh. Da die erste Schicht (5 Neuronen) nicht genug Information liefert, kann die zweite Schicht nichts Nützliches daraus bauen.\n",
    "\n",
    "### 4. Korrektheit der Klassifizierung\n",
    "* **Kreis:** Ja, absolut fehlerfrei in allen Testfällen.\n",
    "* **Spirale:** Nein. Die Klassifizierung ist kaum besser als Zufall.\n",
    "\n",
    "### 5. Analyse der Hidden Layers (Versteckte Schichten)\n",
    "* **Schicht 1:** Diese 5 Neuronen schneiden den Raum immer noch nur mit 5 geraden Linien.\n",
    "* **Schicht 2:** Diese Neuronen kombinieren die 5 Linien zu Polygonen oder offenen Formen.\n",
    "* **Das Flaschenhals-Problem:** Das Versagen bei der Spirale liegt an der ersten Schicht. Da hier nur 5 Linien entstehen, kann die zweite Schicht (egal wie klug sie ist) daraus keine Spirale mit vielen Windungen bauen. Man bräuchte in der ersten Schicht mehr Neuronen, um genug \"Baumaterial\" für die Spirale zu haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e92e38",
   "metadata": {},
   "source": [
    "# Vergleichsanalyse: 3 Hidden Layers (7 Neuronen pro Schicht)\n",
    "\n",
    "### 1. Wie verhält sich die Entscheidungsgrenze?\n",
    "* **Strukturelle Tiefe:** Mit 3 Schichten und insgesamt 21 Neuronen kann das Netzwerk komplexe, hierarchische Strukturen lernen (Linien $\\rightarrow$ Ecken $\\rightarrow$ gewundene Pfade).\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **ReLU & Tanh:** Erzeugen eine perfekte, eng anliegende Hülle um die blauen Punkte.\n",
    "    * **Sigmoid:** Scheitert hier interessantweise (Loss 0.503). Die Grenze bleibt ein diffuser Kreis, der die Klassen nicht trennt. Dies ist ein Indiz für das \"Vanishing Gradient Problem\" (verschwindende Gradienten), das bei tiefen Netzen mit Sigmoid oft auftritt.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **Durchbruch:** Dies ist die erste Konfiguration, die die Spirale ansatzweise erkennt.\n",
    "    * **ReLU:** Bildet einen zackigen, gewundenen \"Schlauch\", der den Spiralarmen folgt.\n",
    "    * **Tanh:** Versucht eine glattere Spirale zu formen, ist aber an den Rändern noch unsauber.\n",
    "    * **Sigmoid:** Versagt komplett (nur eine einfache Kurve), da das Signal durch die vielen Schichten zu schwach wird.\n",
    "\n",
    "### 2. Trainings- und Testkosten & Überanpassung\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **Ergebnis:** Bei ReLU und Tanh nahezu 0.00 Fehler (Loss ~0.001–0.004). Perfekter Fit.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **Fortschritt:** Der Loss sinkt erstmals signifikant unter das Raten (auf ca. 0.350 bei ReLU und 0.382 bei Tanh).\n",
    "    * **Bewertung:** Es ist immer noch ein gewisses Underfitting vorhanden (die Spirale wird nicht perfekt getrennt), aber das Modell hat die grundlegende Struktur \"verstanden\".\n",
    "\n",
    "### 3. Geschwindigkeit der Berechnung\n",
    "* **Beobachtung:** Das Training dauert länger, um gute Ergebnisse zu erzielen.\n",
    "* **Problem bei Sigmoid:** Das Modell lernt fast gar nicht (Stagnation), da die Gewichts-Updates in tiefen Schichten extrem klein werden.\n",
    "\n",
    "### 4. Korrektheit der Klassifizierung\n",
    "* **Kreis:** Perfekt (außer bei Sigmoid).\n",
    "* **Spirale:** Wesentlich besser als zuvor. Die inneren Arme der Spirale werden bei ReLU und Tanh größtenteils korrekt klassifiziert. Fehler treten vor allem an den extremen Außenrändern der Spirale auf.\n",
    "\n",
    "### 5. Analyse der Hidden Layers (Versteckte Schichten)\n",
    "* **Hierarchie:**\n",
    "    * **Schicht 1:** Lernt immer noch nur einfache Geraden.\n",
    "    * **Schicht 2:** Kombiniert diese zu Ecken und einfachen Kurven.\n",
    "    * **Schicht 3:** Kombiniert die Kurven zu komplexen, schlauchigen Pfaden.\n",
    "* **Fazit:** Man sieht hier deutlich, warum \"Tiefe\" (mehr Layer) für komplexe Probleme wie die Spirale wichtiger ist als nur \"Breite\" (mehr Neuronen in einer Schicht). Nur durch die Tiefe kann die komplexe Topologie der Spirale modelliert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df127f",
   "metadata": {},
   "source": [
    "# Vergleichsanalyse: 4 Hidden Layers (7 Neuronen pro Schicht)\n",
    "\n",
    "### 1. Wie verhält sich die Entscheidungsgrenze?\n",
    "* **Strukturelle Tiefe:** Mit 4 Schichten steigt die Komplexität enorm. Theoretisch können sehr komplizierte Formen gebildet werden, praktisch treten aber Trainingsprobleme auf.\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **ReLU:** Bildet eine sehr scharfe, fast hexagonale oder polygonale geschlossene Form um die blauen Punkte.\n",
    "    * **Tanh:** Erzeugt eine sehr glatte, perfekte Kreisform.\n",
    "    * **Sigmoid:** Versagt hier komplett. Die Entscheidungsgrenze zieht sich kaum zusammen. Das Netzwerk ist \"zu tief\" für Sigmoid, das Signal geht verloren.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **ReLU:** Schafft es als einzige Funktion, der Spirale grob zu folgen (zackiger Schlauch), wirkt aber instabil.\n",
    "    * **Tanh & Sigmoid:** Versagen beide. Tanh bildet eher konzentrische Kreise statt einer Spirale, Sigmoid lernt gar nichts (alles wird einer Klasse zugeordnet).\n",
    "\n",
    "### 2. Trainings- und Testkosten & Überanpassung\n",
    "* **Datensatz \"Kreis\":**\n",
    "    * **Ergebnis:** Perfekt für ReLU und Tanh (Loss ~0.001).\n",
    "    * **Problem:** Sigmoid bleibt bei einem hohen Loss (~0.506) stecken. Das ist reines Raten.\n",
    "* **Datensatz \"Spirale\":**\n",
    "    * **Verschlechterung:** Interessanterweise sind die Ergebnisse hier oft schlechter als mit 3 Schichten.\n",
    "    * **Werte:** ReLU erreicht ca. 0.387 (akzeptabel, aber Underfitting). Tanh (~0.486) und Sigmoid (~0.506) stagnieren komplett.\n",
    "\n",
    "### 3. Geschwindigkeit der Berechnung\n",
    "* **Phänomen:** Das Training dauert länger oder friert komplett ein.\n",
    "* **Vanishing Gradient Problem:** Bei 4 Schichten zeigt sich massiv das Problem verschwindender Gradienten. Besonders bei Sigmoid (und hier auch Tanh) kommen die Fehlerkorrekturen nicht mehr in den ersten Schichten an. Die Gewichte werden nicht aktualisiert.\n",
    "\n",
    "### 4. Korrektheit der Klassifizierung\n",
    "* **Kreis:** Ja, perfekt gelöst durch ReLU und Tanh. Sigmoid scheitert.\n",
    "* **Spirale:** Nein. Selbst ReLU klassifiziert viele Randbereiche falsch. Tanh und Sigmoid sind hier unbrauchbar. Mehr Schichten haben hier ohne weitere Hilfsmittel (wie Batch Normalization) geschadet statt genützt.\n",
    "\n",
    "### 5. Analyse der Hidden Layers (Versteckte Schichten)\n",
    "* **Beobachtung:**\n",
    "    * **Schicht 1 & 2:** Bei Sigmoid sieht man fast keine Aktivierungsmuster mehr; die Neuronen sind \"tot\" oder gesättigt.\n",
    "    * **Tiefe vs. Trainierbarkeit:** Das Experiment zeigt deutlich, dass \"tiefer\" nicht automatisch \"besser\" heißt. Ohne moderne Aktivierungsfunktionen (wie ReLU) ist es extrem schwer, Informationen durch 4 Schichten zu transportieren. ReLU hält die Gradienten aktiv, weshalb es hier als einziges noch lernt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b437a0",
   "metadata": {},
   "source": [
    "# 03:\n",
    "# Analyse: Einfluss von Rauschen (Noise = 15) & Überanpassung\n",
    "\n",
    "### 1. Wann spricht man von Überanpassung (Overfitting)?\n",
    "* **Definition:** Überanpassung tritt auf, wenn das Modell das \"Rauschen\" (zufällige Fehler oder Ausreißer im Datensatz) statt des eigentlichen Musters (\"Signal\") lernt.\n",
    "* **Visuelles Merkmal:** Die Entscheidungsgrenze wird unnötig komplex. Sie bildet \"Inseln\", Schlaufen oder sehr zackige Linien, nur um einzelne Ausreißer-Punkte (z.B. einen einzelnen blauen Punkt tief im orangen Gebiet) korrekt zu klassifizieren.\n",
    "* **Metrisches Merkmal:**\n",
    "    * Der **Training Loss** sinkt weiter (das Modell \"merkt\" sich die Ausreißer).\n",
    "    * Der **Test Loss** beginnt zu steigen oder stagniert auf einem deutlich schlechteren Niveau als der Training Loss. Die Schere zwischen Training- und Test-Kurve geht auseinander.\n",
    "\n",
    "### 2. Beobachtungen in den Experimenten (mit Noise 15)\n",
    "* **Bei geringer Komplexität (z.B. 2-3 Neuronen):**\n",
    "    * **Verhalten:** Das Modell ist zu starr (\"High Bias\"). Es ignoriert das Rauschen zwangsläufig, da es keine komplexen Grenzen bilden kann. Es entsteht *kein* Overfitting, sondern weiterhin Underfitting.\n",
    "* **Bei hoher Komplexität (z.B. 4 Schichten, viele Neuronen):**\n",
    "    * **Verhalten:** Das Netzwerk hat genug Kapazität, um die verstreuten Punkte einzeln einzukreisen.\n",
    "    * **Ergebnis:** Die Entscheidungsgrenze sieht zerklüftet aus. Das Modell generalisiert schlecht auf neue Daten (Testdaten), da es sich zu sehr auf die spezifische Verteilung der Trainingsdaten inkl. ihrer Fehler versteift hat.\n",
    "\n",
    "### 3. Fazit\n",
    "* **Erkenntnis:** Rauschen zwingt uns, die Komplexität des Modells zu begrenzen (Regularisierung) oder das Training rechtzeitig zu stoppen (Early Stopping), um zu verhindern, dass das Netz \"Phantome\" jagt. Eine glattere Grenze, die ein paar Fehler in Kauf nimmt, ist bei Noise=15 meist besser als eine perfekte Trennung der Trainingsdaten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
